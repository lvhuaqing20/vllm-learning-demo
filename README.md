# vLLM Deployment on School Server

This repository contains scripts and notes for deploying vLLM on a server with CUDA 11.8 (NVIDIA RTX 3090).

## Prerequisites
- CUDA 12.4
- Python 3.12
- GPU: RTX 3090 (24GB)

## Quick Start
1. Install dependencies:
   ```bash
   pip install -r requirements.txt
