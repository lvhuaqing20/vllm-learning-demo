import streamlit as st
import re
from vllm import LLM, SamplingParams
import os

# -----------------------------------------------------------------------------
# 1. åŸºç¡€é…ç½®åŒº
# -----------------------------------------------------------------------------
st.set_page_config(page_title="Qwen-0.5B åŒæ¨¡å¼æ¼”ç¤º", layout="wide")
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# ä½ çš„æ¨¡å‹ç»å¯¹è·¯å¾„ (è¯·ç¡®è®¤è¿™ä¸ªè·¯å¾„æ˜¯æ­£ç¡®çš„)
MODEL_PATH = "/home/leijianuo/.cache/huggingface/hub/input0"

# -----------------------------------------------------------------------------
# 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
# -----------------------------------------------------------------------------

# å›è°ƒå‡½æ•°ï¼šåˆ‡æ¢æ¨¡å¼æ—¶æ¸…ç©ºèŠå¤©è®°å½•ï¼Œé˜²æ­¢æ¨¡å‹â€œå­¦åâ€
def reset_chat():
    st.session_state.messages = []

@st.cache_resource
def load_model():
    print(f"æ­£åœ¨ä» {MODEL_PATH} åŠ è½½æ¨¡å‹...")
    # æ˜¾å­˜é˜²å´©æºƒé…ç½®
    llm = LLM(
        model=MODEL_PATH,
        trust_remote_code=True,
        max_model_len=8192,         # ã€ä¿®æ”¹ã€‘è„‘å®¹é‡æ‰©å……åˆ° 4096
        gpu_memory_utilization=0.6  # ã€ä¿®æ”¹ã€‘åªå  40% æ˜¾å­˜ï¼Œé˜²æ­¢ OOM
    )
    return llm

# å°è¯•åŠ è½½æ¨¡å‹
try:
    with st.spinner("æ­£åœ¨å”¤é†’ Qwen æ¨¡å‹ï¼Œè¯·ç¨å€™..."):
        llm = load_model()
    st.success("æ¨¡å‹åŠ è½½å°±ç»ªï¼")
except Exception as e:
    st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {e}")

# -----------------------------------------------------------------------------
# 3. ä¾§è¾¹æ  (UI)
# -----------------------------------------------------------------------------
with st.sidebar:
    st.title("ğŸ§  æ€è€ƒæ¨¡å¼")
    
    # æ¨¡å¼é€‰æ‹© (ç»‘å®šäº† on_change=reset_chatï¼Œä¸€æ¢æ¨¡å¼å°±æ¸…å±)
    mode = st.radio(
        "é€‰æ‹© System:",
        ("å¿«æ€è€ƒ (System 1)", "æ…¢æ€è€ƒ (System 2)"),
        captions=["ç›´è§‰ååº”ï¼Œé€Ÿåº¦å¿«", "æ·±åº¦æ¨ç†ï¼Œé€»è¾‘å¼º"],
        on_change=reset_chat 
    )
    
    st.markdown("---")
    st.markdown("### ğŸ’¡ æµ‹è¯•å»ºè®®")
    st.markdown("- **å¿«æ¨¡å¼**: 9.11 å’Œ 9.9 å“ªä¸ªå¤§ï¼Ÿ")
    st.markdown("- **æ…¢æ¨¡å¼**: Strawberry æœ‰å‡ ä¸ª rï¼Ÿ")
    
    st.markdown("---")
    # æ‰‹åŠ¨æ¸…ç©ºæŒ‰é’®
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯"):
        reset_chat()
        st.rerun()

# -----------------------------------------------------------------------------
# 4. ä¸»èŠå¤©ç•Œé¢
# -----------------------------------------------------------------------------
st.title("ğŸ¤– Qwen-0.5B å¿«æ…¢æ€è€ƒåŒæ¨¡å¼")

# åˆå§‹åŒ– Session State
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²èŠå¤©è®°å½•
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# -----------------------------------------------------------------------------
# 5. å¤„ç†ç”¨æˆ·è¾“å…¥ä¸ç”Ÿæˆ
# -----------------------------------------------------------------------------
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    # A. æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # B. ç”Ÿæˆå›ç­”
    with st.chat_message("assistant"):
        # æ ¹æ®æ¨¡å¼è®¾å®š System Prompt å’Œ å‚æ•°
        if mode == "å¿«æ€è€ƒ (System 1)":
            # å¿«æ¨¡å¼ï¼šæ¸©åº¦0ï¼Œå¼ºåˆ¶ç®€çŸ­
            temp = 0.0
            max_tk = 5000  # ã€ä¿®æ”¹ã€‘å¢åŠ åˆ° 512ï¼Œé˜²æ­¢è¯æ²¡è¯´å®Œ
            sys_prompt = "You are a concise assistant. Answer the user's question directly. Do NOT use <think> tags. Do not explain your reasoning."
        else:
            # æ…¢æ¨¡å¼ï¼šæ¸©åº¦0.6ï¼Œå¼ºåˆ¶ CoT (Chain of Thought)
            temp = 0.6
            max_tk = 7000 # ã€ä¿®æ”¹ã€‘å¢åŠ åˆ° 3500ï¼Œå…è®¸é•¿ç¯‡å¤§è®º
            sys_prompt = "You are a logical expert. You must think step by step before answering. Use <think> tags for your reasoning."

        # æ„é€  ChatML æ ¼å¼çš„ Prompt
        # æ³¨æ„ï¼šä¸ºäº†æ¼”ç¤ºæ•ˆæœæ¸…æ™°ï¼Œè¿™é‡Œä»…å‘é€å½“å‰å•è½®å¯¹è¯ï¼Œé¿å…å†å²è®°å½•å¹²æ‰° System Prompt çš„æ•ˆæœ
        full_prompt = f"<|im_start|>system\n{sys_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
        
        # è®¾ç½®é‡‡æ ·å‚æ•°
        sampling_params = SamplingParams(
            temperature=temp, 
            top_p=0.8, 
            max_tokens=max_tk, 
            stop=["<|im_end|>"]
        )

        # è°ƒç”¨ vLLM ç”Ÿæˆ (è€—æ—¶æ“ä½œ)
        with st.spinner(f'æ­£åœ¨ä½¿ç”¨ {mode} æ€è€ƒä¸­...'):
            outputs = llm.generate([full_prompt], sampling_params)
            response = outputs[0].outputs[0].text.strip()
# ã€æ–°å¢ã€‘æš´åŠ›æ¸…æ´—é€»è¾‘ï¼šå¦‚æœæ˜¯å¿«æ¨¡å¼ï¼Œå¼ºåˆ¶åˆ é™¤ <think> æ ‡ç­¾åŠå…¶å†…å®¹
        if mode == "å¿«æ€è€ƒ (System 1)":
            # è¿™é‡Œçš„æ­£åˆ™æ„æ€æ˜¯ï¼šæ‰¾åˆ° <think> å’Œ </think> ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ï¼Œæ›¿æ¢ä¸ºç©º
            # flags=re.DOTALL è¡¨ç¤ºå…è®¸åŒ¹é…æ¢è¡Œç¬¦
            response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()

        # æ˜¾ç¤ºå¹¶ä¿å­˜å›ç­”
        st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
