from vllm import LLM, SamplingParams
import os

# 1. è®¾ç½®é•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# 2. é…ç½®æ¨¡å‹è·¯å¾„ (ä½ ç¡®è®¤è¿‡çš„æ­£ç¡®è·¯å¾„)
MODEL_PATH = "/home/leijianuo/.cache/huggingface/hub/input0"

print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ (é™åˆ¶æ˜¾å­˜ä»¥é˜²å´©æºƒ)...")
# ã€æ ¸å¿ƒé…ç½®ã€‘èåˆäº†ä½ çš„æœåŠ¡å™¨é˜²å´©æºƒè®¾ç½®
llm = LLM(
    model=MODEL_PATH,
    trust_remote_code=True,
    max_model_len=2048,         # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
    gpu_memory_utilization=0.4  # åªå  40% æ˜¾å­˜ï¼Œé˜²æ­¢ OOM
)

# ---------------------------------------------------------
# æ¨¡å¼ A: å¿«æ€è€ƒ (System 1) - ç›´è§‰ã€å¿«é€Ÿã€é›¶åºŸè¯
# ---------------------------------------------------------
def run_fast_mode(question):
    print(f"\nğŸš€ [å¿«æ€è€ƒæ¨¡å¼ System 1] é—®é¢˜: {question}")
    
    # æ¸©åº¦è®¾ä¸º 0ï¼Œç»“æœå›ºå®šï¼Œä¸éšæœº
    sampling_params = SamplingParams(temperature=0.0, max_tokens=100, stop=["<|im_end|>"])
    
    # æç¤ºè¯ï¼šè¦æ±‚ç®€çŸ­
    prompt = f"""<|im_start|>system
You are a concise assistant. Answer directly without explanation.<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
"""
    output = llm.generate([prompt], sampling_params)[0]
    print(f"ğŸ‘‰ ç»“æœ: {output.outputs[0].text.strip()}")

# ---------------------------------------------------------
# æ¨¡å¼ B: æ…¢æ€è€ƒ (System 2) - é€»è¾‘ã€æ¨ç†ã€ä¸€æ­¥æ­¥æ¥
# ---------------------------------------------------------
def run_slow_mode(question):
    print(f"\nğŸ¢ [æ…¢æ€è€ƒæ¨¡å¼ System 2] é—®é¢˜: {question}")
    
    # æ¸©åº¦è®¾ä¸º 0.6ï¼Œå…è®¸ä¸€ç‚¹åˆ›é€ æ€§æ€ç»´
    sampling_params = SamplingParams(temperature=0.6, max_tokens=1024, stop=["<|im_end|>"])
    
    # æç¤ºè¯ï¼šå¼ºåˆ¶è¦æ±‚ä¸€æ­¥æ­¥æ€è€ƒ (Chain of Thought)
    prompt = f"""<|im_start|>system
You are a logical expert. You must think step by step before answering. Use <think> tags for your reasoning.<|im_end|>
<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant
"""
    output = llm.generate([prompt], sampling_params)[0]
    print(f"ğŸ§  ç»“æœ: {output.outputs[0].text.strip()}")

# ---------------------------------------------------------
# æµ‹è¯•ä¸»ç¨‹åº
# ---------------------------------------------------------
if __name__ == "__main__":
    # ä¸€ä¸ªç»å…¸çš„é€»è¾‘é™·é˜±é¢˜ï¼Œå¿«æ€è€ƒå®¹æ˜“é”™ï¼Œæ…¢æ€è€ƒå®¹æ˜“å¯¹
    question = "å¦‚æœä¸è€ƒè™‘é—°å¹´ï¼Œä¸€å¹´é‡Œæœ‰å‡ ä¸ªæœˆæœ‰28å¤©ï¼Ÿ"
    
    # 1. è¿è¡Œå¿«æ¨¡å¼
    run_fast_mode(question)
    
    # 2. è¿è¡Œæ…¢æ¨¡å¼
    run_slow_mode(question)
